{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, re, sys, os, ssl, unicodedata, itertools, lxml, bs4, requests, multiprocessing\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from multiprocessing.pool import Pool\n",
    "from grobid_client_python import grobid_client as grobid\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "##########\n",
    "# This script allows for conversion of pdfs to an excel spreadsheet, parsing out text by keywords from each file\n",
    "# Written/modified by Corinn Small\n",
    "#########\n",
    "\n",
    "\n",
    "def convert_text(inpath,outpath):\n",
    "    \n",
    "    '''\n",
    "    converts text from pdf to tei.xml using grobid web service api, remember computer has to be connected to the server: cd grobid-0.6.1/ -> ./gradlew run\n",
    "    input: path to papers\n",
    "    output: tei.xml file per pdf\n",
    "    '''\n",
    "\n",
    "    print('Converting text...')\n",
    "    print('')\n",
    "    \n",
    "    \n",
    "    client = grobid.grobid_client(config_path=\"./grobid_client_python/config.json\")\n",
    "    client.process(\"processFulltextDocument\", inpath, outpath)\n",
    "    \n",
    "    print('Done')\n",
    "\n",
    "def read_tei(tei_file):\n",
    "    '''\n",
    "    Reads in an xml file and returns a beautifulsoup object\n",
    "    \n",
    "    '''\n",
    "    with open(tei_file, 'r') as tei:\n",
    "        soup = BeautifulSoup(tei, 'html.parser')\n",
    "        return soup\n",
    "    raise RuntimeError('Cannot generate a soup from the input')\n",
    "    \n",
    "\n",
    "def elem_to_text(elem, default='NA'):\n",
    "    '''\n",
    "    Returns element if it exists, if not returns NA\n",
    "    \n",
    "    '''\n",
    "    if elem:\n",
    "        return elem.getText()\n",
    "    else:\n",
    "        return default\n",
    "    \n",
    "\n",
    "#create class for storing pdf info\n",
    "\n",
    "class TEIFile(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.soup = read_tei(filename)  #creates soup object\n",
    "        self._text = None\n",
    "        self._title = ''  \n",
    "        self._abstract = ''\n",
    "        self._keytext = {}\n",
    "        self._doublecheck = None\n",
    "        \n",
    "    @property\n",
    "    def doi(self, id_='DOI'):\n",
    "        '''\n",
    "        retrieve id\n",
    "        '''\n",
    "    \n",
    "        idno_elem = self.soup.find('idno', type='DOI')\n",
    "        if not idno_elem:\n",
    "            return 'no id'\n",
    "        else:\n",
    "            return idno_elem.getText()\n",
    "    \n",
    "    @property\n",
    "    def title(self):\n",
    "        '''\n",
    "        retrieve title\n",
    "        '''\n",
    "        if not self._title:\n",
    "            self._title = self.soup.title.getText()\n",
    "        return self._title\n",
    "    \n",
    "    @property\n",
    "    def abstract(self):\n",
    "        '''\n",
    "        retrieve abstract\n",
    "        '''\n",
    "        \n",
    "        if not self._abstract:\n",
    "            abstract = self.soup.abstract.getText(separator=' ', strip=True)\n",
    "            self._abstract = abstract\n",
    "        return self._abstract\n",
    "      \n",
    "    @property\n",
    "    def authors(self):\n",
    "        '''\n",
    "        retrieve authors\n",
    "        '''\n",
    "        authors_in_header = self.soup.analytic.find_all('author')\n",
    "\n",
    "        result = []\n",
    "        \n",
    "        @dataclass\n",
    "        class Person:\n",
    "            firstname: str\n",
    "            middlename: str\n",
    "            surname: str\n",
    "                \n",
    "        for author in authors_in_header:\n",
    "            persname = author.persname\n",
    "            if not persname:\n",
    "                continue\n",
    "            firstname = elem_to_text(persname.find(\"forename\", type=\"first\"))\n",
    "            middlename = elem_to_text(persname.find(\"forename\", type=\"middle\"))\n",
    "            surname = elem_to_text(persname.surname)\n",
    "            person = Person(surname, firstname, middlename)\n",
    "            result.append(person)\n",
    "        return result\n",
    "    \n",
    "    @property\n",
    "    def text(self):\n",
    "        '''\n",
    "        retrieves text\n",
    "        returns dictionary by subsection\n",
    "        '''\n",
    "        #print(self.soup.prettify())\n",
    "        print(self.filename.split('/')[-1])\n",
    "        if not self._text:\n",
    "            divs_text = {}\n",
    "            \n",
    "            for div in self.soup.body.find_all('div'): \n",
    "                \n",
    "                if not div.get('type'):  # div is neither an appendix nor references, just plain text.\n",
    "                    heads = div.find_all('head')\n",
    "                    \n",
    "                    if not heads:\n",
    "                        self._doublecheck = True\n",
    "                        div_text = []\n",
    "                        p_text = div.get_text(separator=' ', strip=True)\n",
    "                        div_text.append(p_text)\n",
    "                        divs_text['body'] = div_text\n",
    "                        \n",
    "                    else:\n",
    "                        sect = []\n",
    "                        \n",
    "                        for head in heads:\n",
    "                            head_text = head.get_text(separator=' ', strip=True).lower()\n",
    "                            #print(head_text)\n",
    "                            \n",
    "                            if head_text.find('fig') == -1:  #exclude figure \n",
    "                                print(head_text)\n",
    "                                \n",
    "                                for p in div.find_all('p'):\n",
    "                                    #print(p)\n",
    "                                    sect_text = p.get_text(separator=' ', strip=True)\n",
    "                                    sect.append(sect_text)\n",
    "                                    divs_text[head.get_text()] = sect\n",
    "                    \n",
    "            self._text = divs_text\n",
    "        \n",
    "        return self._text\n",
    "    \n",
    "    \n",
    "    def keytext(self, keywords):\n",
    "        '''\n",
    "        retrieves sentences by keyword\n",
    "        returns dictionary\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        for keyword in keywords:\n",
    "            self._keytext[keyword] = []\n",
    "            \n",
    "        \n",
    "        for k,v in self._text.items():  #for subsection and text \n",
    "                for keyword in keywords:  #search thru sentences \n",
    "                    sub_dict = {}\n",
    "                    section = k.lower()\n",
    "                    sub_dict[section] = []  #create list for each subsection in dictionary\n",
    "                    \n",
    "                    for i in v:  #for paragraph in text\n",
    "                        sentences = sent_tokenize(i)  #get list of sentences\n",
    "                        sentences = [s.lower() for s in sentences]\n",
    "                    \n",
    "                        for sentence in sentences:\n",
    "                            result = re.findall('\\\\b' + keyword + '\\\\b', sentence)  #find keyword in sentence\n",
    "\n",
    "                            if len(result) > 0:  #if keyword exists, \n",
    "                                sub_dict[section].append(sentence)  #add sentence to subsection list\n",
    "\n",
    "                            else:  \n",
    "                                pass\n",
    "                    \n",
    "                    #print(sub_dict)\n",
    "                    self._keytext[keyword].append(sub_dict)  #adds each subsection to keyword dictionary\n",
    "                        \n",
    "        return self._keytext                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get input from user\n",
    "\n",
    "cases = 'single'\n",
    "\n",
    "path = '/Users/corinnsmall/Documents/BPDCN/bpdcn_papers/' + cases +'_case_papers/'\n",
    "papers = '*.xml'\n",
    "keywords_ = 'morphologies'\n",
    "output = 'output_' + cases + '_cases_' + keywords_\n",
    "\n",
    "associated_diseases_keywords = ['acute myeloid leukemia', 'AML', 'acute lymphoblastic leukemia', 'ALL', 'leukemia', 'non-hodgkin lymphoma', 'Hodgkin lymphoma',\n",
    "                                'lymphoma','myelogenous leukemia', 'multiple myeloma', 'myeloma', 'chronic myelogenous leukemia', 'CML', 'chronic lymphocytic leukemia', \n",
    "                                'CLL', 'carcinoma','Pleuropulmonary blastoma', 'blastoma', 'neuroblastoma', 'melanoma', 'sarcoma', 'skin cancer', 'hairy cell leukemia',\n",
    "                                'ependymoma', 'chordoma','bone cancer', 'bladder', 'AIDS-related lymphoma', 'thyroid cancer', 'colon cancer', 'rectal cancer',\n",
    "                                'prostate cancer', 'chronic myeloid leukemia', 'myeloproliferative', 'myelodysplastic', 'mast cell', 'mastocytosis', 'lymphoblastic', \n",
    "                                'follicular lymphoma', 'marginal zone lymphoma', 'langerhan', 'polycythemia vera', 'essential thrombocythemia', 'myelofibrosis', \n",
    "                                'mycosis fungoides', 'sezary', 'burkitt', 'cmml', 'chronic myelomonocytic leukemia']\n",
    "\n",
    "morphology_keywords = ['Vacuoles','Vacuolated','Microvacuoles','Lymphoid','Eccentrically','Eccentric','Prominent nucleoli', 'Small nucleoli','Large nucleoli',\n",
    "                       'Medium nucleoli','azurophilic','Blast','Blastoid','Agranular','Basophilic','Eosinophilic','Perivascular','Periadnexal','Pseudopodia','Hairy','Rosary beads',\n",
    "                       'Large nucleolus','Monoblastic','Monocytic','Histiocytic','Histiocytoid','Small granules','Large granules','Granulated','Granular','Condensed chromatin',\n",
    "                       'Dispersed chromatin','Fine chromatin','Pale cytoplasm','Poorly differentiated', 'Large sized','Medium sized','Small sized','Plasmablast', \n",
    "                       'Plasmacytoid','plasmacytic','Immature','lymphoblast']\n",
    "\n",
    "keylist = morphology_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'paper_folder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-bf73f6793bce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#convert pdfs to xml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0moutpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/corinnsmall/Documents/BPDCN/bpdcn_papers/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpaper_folder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'xml_output/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mconvert_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'paper_folder' is not defined"
     ]
    }
   ],
   "source": [
    "#convert pdfs to xml\n",
    "\n",
    "outpath = path + 'xml_output/'\n",
    "convert_text(path,outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "munoz_2011.tei.xml\n",
      "[]\n",
      "['Sixty-six-year-old female presented with left forearm soft tissue swelling for 2 months. An ultrasound of her left forearm showed a 6.5 cm 3 1 cm 3 3.6 cm soft tissue mass, which was also confirmed by magnetic resonance imaging (Panel A and B). A routine screening mammogram revealed a new 4 mm mass in her left breast. Subsequent left breast needle biopsy and left forearm fine needle aspiration favored an undifferentiated malignant neoplasm of hematolymphoid origin (Panel D) with positive CD4 (Panel E) and positive CD56 (Panel F) markers compatible with blastic plasmacytoid dendritic cell neoplasm (BPDCN). Positron emission tomography (PET) scan showed increased uptake in the left forearm (Panel C), left breast, bilateral pleura, liver, spleen, portocaval lymph nodes, and omental caking. A bone marrow biopsy was negative for malignancy, and shortly thereafter the patient underwent induction therapy with cytarabine and idarubicin. Following progression of metastatic disease on PET scan, the patient received reinduction chemotherapy with mitoxantrone, etoposide, and cytarabine.', 'BPDCN is a rare malignancy formerly recognized as CD41/CD561 hematodermic neoplasm and is suggested to have derived from plasmacytoid dendritic cells. It typically presents in middle-aged or elderly patients with skin or soft tissue involvement and concurrent disseminating disease carrying poor prognosis. BPDCN expresses CD4, CD56, CD123 (interleukin-3 receptor alpha chain), and BDCA-2 (blood dendritic cell antigen 2), whereas expression of TCL1 is helpful when the tissue displays weak expression of aforementioned markers. Treatment involves multiagent chemotherapy followed by subsequent stem cell transplantation for best outcomes. Unfortunately, our patient showed dismal response to chemotherapy, and the originally intended stem cell transplantation could not be instituted due to poor performance status.']\n",
      "yunningyang_2019.tei.xml\n",
      "[<head>Introduction</head>]\n",
      "introduction\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'sect' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-35d8796df681>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTEIFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     '''f.keytext(keylist)\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-0bf05f08ceeb>\u001b[0m in \u001b[0;36mtext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m                                     \u001b[0;31m#print(p)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                                     \u001b[0msect_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseparator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                                     \u001b[0msect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msect_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m                                     \u001b[0mdivs_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'sect' referenced before assignment"
     ]
    }
   ],
   "source": [
    "#get .xml files from path, convert pdf to TEIFILE object\n",
    "#get dictionary of keywords and sentences\n",
    "#create formatted lists for creation of dataframe\n",
    "\n",
    "files = glob.glob(path + 'xml_output/' + papers)\n",
    "files_list = []\n",
    "\n",
    "for file in files:\n",
    "    #print(file)\n",
    "    file_list = []\n",
    "    f = TEIFile(file)\n",
    "    name = f.filename.split('/')[-1].split('.')[0]\n",
    "    f.text\n",
    "    '''f.keytext(keylist)\n",
    "    \n",
    "    for k,v in f._keytext.items():  \n",
    "        for d in v:\n",
    "            for i,j in d.items():\n",
    "                \n",
    "                if len(j) == 0 and f._doublecheck == None:\n",
    "                    file_list = [name, f.filename, f.doi, 'NA', k, i, 'NA']\n",
    "        \n",
    "                elif len(j) == 0 and f._doublecheck == True:\n",
    "                    file_list = [name, f.filename, f.doi, f._doublecheck, k, i, 'NA']\n",
    "                    \n",
    "                elif len(i.split(' ')) > 10:  #check for whether section title has more than 10 words, and flags it if so\n",
    "                    f._doublecheck = True\n",
    "                    file_list = [name, f.filename, f.doi, f._doublecheck, k, i, 'NA']\n",
    "\n",
    "                elif f._doublecheck == None:\n",
    "                    file_list = [name, f.filename, f.doi, 'NA', k, i, j]   \n",
    "\n",
    "                else:\n",
    "                    file_list = [name, f.filename, f.doi, f._doublecheck, k, i, j]\n",
    "\n",
    "                files_list.append(file_list)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Ahoy!\n"
     ]
    }
   ],
   "source": [
    "#create dataframe and write to excel file\n",
    "\n",
    "df = pd.DataFrame(files_list, columns = ['pdf_name','pdf_location','doi', 'doublecheck paper?','keywords', 'section', 'text'])\n",
    "                \n",
    "try:\n",
    "    with open(path + output + '.xlsx', 'wb') as out:\n",
    "        df.to_excel(out)\n",
    "        \n",
    "    if out.closed:\n",
    "        print('Data Ahoy!')\n",
    "\n",
    "except IOError:\n",
    "        print('I/O error')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
